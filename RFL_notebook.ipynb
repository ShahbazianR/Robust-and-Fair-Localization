{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Robust & Fair Localization (RFL)\n",
        "\n",
        "Reproducible simulation and plots for the Robust Fair Localization method.\n",
        "- One chart per figure; default Matplotlib colors.\n",
        "- Fixed colors and unique markers for each method.\n",
        "- Learning prior + damped Gauss\u2013Newton + confidence-weighted consensus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom dataclasses import dataclass\n\nrng = np.random.default_rng\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef generate_anchors(num_anchors=12, area_size=180.0, seed=321):\n    r = area_size * 0.4\n    g = rng(seed)\n    angles = np.linspace(0, 2*np.pi, num_anchors, endpoint=False)\n    ring = np.stack([\n        area_size/2 + (r + g.normal(0, area_size*0.01)) * np.cos(angles),\n        area_size/2 + (r + g.normal(0, area_size*0.01)) * np.sin(angles)\n    ], axis=1)\n    return ring\n\ndef generate_points(num_points=3000, area_size=180.0, seed=123):\n    g = rng(seed)\n    return g.uniform(0, area_size, size=(num_points, 2))\n\ndef simulate_rssi(anchors, points, noise_sigma=2.0, seed=0, A0=-30.0, n=2.0, d0=1.0):\n    g = rng(seed)\n    dists = np.linalg.norm(points[:, None, :] - anchors[None, :, :], axis=2) + 1e-6\n    rss = A0 - 10.0 * n * np.log10(dists / d0)\n    rss += g.normal(0.0, noise_sigma, size=rss.shape)\n    return rss.astype(np.float32)\n\ndef invert_rssi_to_ranges(rssi, A0=-30.0, n=2.0, d0=1.0):\n    return d0 * (10.0 ** ((A0 - rssi) / (10.0 * n)))\n\ndef rmse(true_xy, est_xy):\n    e = np.linalg.norm(true_xy - est_xy, axis=1)\n    return float(np.sqrt(np.mean(e**2)))\n\ndef fairness_phi(true_xy, est_xy):\n    e = np.linalg.norm(true_xy - est_xy, axis=1)\n    mu, var = np.mean(e), np.var(e)\n    if mu <= 1e-12:\n        return 1.0\n    return float(1.0 - (var / (mu**2)))\n\ndef robustness_rho(sigmas, rmses):\n    sigmas = np.array(sigmas, dtype=float)\n    rmses = np.array(rmses, dtype=float)\n    return float(np.mean(1.0 / (rmses * sigmas)))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models and Core Solvers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n@dataclass\nclass KNNEnsemble:\n    k: int = 5\n    n_models: int = 2\n    noise_sigma: float = 2.0\n    seed: int = 0\n\n    def fit(self, X, Y):\n        g = rng(self.seed)\n        self.models, self.Ys = [], []\n        N = X.shape[0]\n        for _ in range(self.n_models):\n            idx = g.integers(0, N, size=N)\n            Xm = X[idx] + g.normal(0, self.noise_sigma, size=X[idx].shape)\n            Ym = Y[idx]\n            nbrs = NearestNeighbors(n_neighbors=self.k).fit(Xm)\n            self.models.append(nbrs)\n            self.Ys.append(Ym)\n        return self\n\n    def predict(self, X):\n        preds = []\n        for nbrs, Ym in zip(self.models, self.Ys):\n            d, idx = nbrs.kneighbors(X, return_distance=True)\n            w = 1.0 / (d + 1e-6)\n            w = w / w.sum(axis=1, keepdims=True)\n            est = (w[..., None] * Ym[idx]).sum(axis=1)\n            preds.append(est)\n        return np.mean(preds, axis=0)\n\ndef tukey_weights(r, c):\n    x = r / c\n    w = (1 - x**2)**2\n    w[np.abs(x) >= 1] = 0.0\n    return w\n\ndef robust_gn(anchors, ranges_row, x_init, iters=16, c=2.5, damping=1e-3, prior=None, lam_prior=0.3):\n    x = x_init.astype(float).copy()\n    A = anchors; r = ranges_row\n    for _ in range(iters):\n        diff = x[None, :] - A\n        d = np.linalg.norm(diff, axis=1) + 1e-9\n        resid = d - r\n        J = diff / d[:, None]\n        s = np.median(np.abs(resid)) + 1e-9\n        w = tukey_weights(resid / (s + 1e-12), c)\n        W = np.diag(w)\n        H = J.T @ W @ J + (damping * np.eye(2))\n        b = J.T @ (W @ resid)\n        if prior is not None and lam_prior > 0.0:\n            H = H + lam_prior * np.eye(2)\n            b = b + lam_prior * (x - prior)\n        step = -np.linalg.solve(H, b)\n        x = x + step\n    return x\n\ndef consensus_confidence(estimates, conf, k=10, alpha=0.4, iters=14):\n    import numpy as _np  # guard\n    est = estimates.astype(_np.float64).copy()\n    conf = conf.reshape(-1).astype(_np.float64)\n    nbrs = NearestNeighbors(n_neighbors=min(k, len(est))).fit(est)\n    for _ in range(iters):\n        d, idx = nbrs.kneighbors(est, return_distance=True)\n        w = 1.0 / (d + 1e-6)\n        w = w / w.sum(axis=1, keepdims=True)\n        neighbor_avg = (w[..., None] * est[idx]).sum(axis=1)\n        est = est + (alpha * conf)[:, None] * (neighbor_avg - est)\n        nbrs = NearestNeighbors(n_neighbors=min(k, len(est))).fit(est)\n    return est.astype(np.float32)\n\ndef multilateration_opt(anchors, ranges):\n    # Baseline: no prior\n    w = 1.0 / (ranges**2 + 1e-9)\n    w = w / w.sum(axis=1, keepdims=True)\n    x0 = (w[..., None] * anchors[None, :, :]).sum(axis=1)\n    out = np.zeros_like(x0)\n    for i in range(ranges.shape[0]):\n        out[i] = robust_gn(anchors, ranges[i], x0[i], iters=10, lam_prior=0.0)\n    return out\n\ndef rfl_estimator(anchors, ranges, prior_xy):\n    # Robust GN with stronger prior toward kNN\n    gn = np.zeros_like(prior_xy)\n    for i in range(ranges.shape[0]):\n        gn[i] = robust_gn(anchors, ranges[i], x_init=prior_xy[i], iters=16, lam_prior=0.40)\n    # Quality-based blending and consensus\n    diff = gn[:, None, :] - anchors[None, :, :]\n    d = np.linalg.norm(diff, axis=2) + 1e-9\n    resid = np.abs(d - ranges)\n    q = 1.0 / (1.0 + resid.mean(axis=1))\n    q = (q - q.min()) / (q.max() - q.min() + 1e-9)\n    blended = (0.75*q)[:, None]*gn + (1 - 0.75*q)[:, None]*prior_xy\n    conf = 0.5 + 0.5*q\n    return consensus_confidence(blended, conf=conf, k=10, alpha=0.45, iters=14)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef run_experiment(seed=321, area_size=180.0, sigmas=(1.0,1.5,2.0,2.5,3.0),\n                   num_anchors=12, num_train=3000, num_test=1200):\n    anchors = generate_anchors(num_anchors=num_anchors, area_size=area_size, seed=seed)\n    train_pts = generate_points(num_points=num_train, area_size=area_size, seed=seed+1)\n    test_pts  = generate_points(num_points=num_test,  area_size=area_size, seed=seed+2)\n\n    train_rssi = simulate_rssi(anchors, train_pts, noise_sigma=3.0, seed=seed+3)\n    ens = KNNEnsemble(k=5, n_models=2, noise_sigma=2.0, seed=seed+10).fit(train_rssi, train_pts)\n\n    rows = []\n    for sig in sigmas:\n        test_rssi = simulate_rssi(anchors, test_pts, noise_sigma=sig, seed=seed+100+int(sig*10))\n        knn_xy = ens.predict(test_rssi)\n        test_ranges = invert_rssi_to_ranges(test_rssi)\n        opt_xy = multilateration_opt(anchors, test_ranges)\n        cons_xy = consensus_confidence(knn_xy, conf=np.ones(len(knn_xy)), k=8, alpha=0.35, iters=12)\n        rfl_xy = rfl_estimator(anchors, test_ranges, prior_xy=knn_xy)\n\n        rows.append({\n            \"sigma\": sig,\n            \"rmse_Optimization\": rmse(test_pts, opt_xy),\n            \"rmse_Consensus\": rmse(test_pts, cons_xy),\n            \"rmse_kNN\": rmse(test_pts, knn_xy),\n            \"rmse_RFL\": rmse(test_pts, rfl_xy),\n            \"fair_Optimization\": fairness_phi(test_pts, opt_xy),\n            \"fair_Consensus\": fairness_phi(test_pts, cons_xy),\n            \"fair_kNN\": fairness_phi(test_pts, knn_xy),\n            \"fair_RFL\": fairness_phi(test_pts, rfl_xy),\n        })\n\n    df = pd.DataFrame(rows)\n\n    rho = {\n        \"rho_opt\": robustness_rho(df[\"sigma\"], df[\"rmse_Optimization\"]),\n        \"rho_cons\": robustness_rho(df[\"sigma\"], df[\"rmse_Consensus\"]),\n        \"rho_knn\":  robustness_rho(df[\"sigma\"], df[\"rmse_kNN\"]),\n        \"rho_rfl\":  robustness_rho(df[\"sigma\"], df[\"rmse_RFL\"]),\n    }\n    return anchors, test_pts, df, rho\n\nanchors, test_pts, df, rho = run_experiment()\n\nprint(\"\\nPer-sigma metrics:\")\nprint(df.round(4))\nprint(\"\\nRobustness rho:\", {k: round(v, 6) for k, v in rho.items()})\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ncolors = {\n    \"Optimization\": \"#1f77b4\",\n    \"Consensus\":    \"#ff7f0e\",\n    \"kNN\":          \"#2ca02c\",\n    \"RFL (Proposed)\":\"#d62728\",\n}\nmarkers = {\n    \"Optimization\": \"o\",\n    \"Consensus\":    \"s\",\n    \"kNN\":          \"^\",\n    \"RFL (Proposed)\":\"D\",\n}\nlinestyles = {\n    \"Optimization\": \"-\",\n    \"Consensus\":    \"--\",\n    \"kNN\":          \"-.\",\n    \"RFL (Proposed)\":\"-\",\n}\n\nx  = df[\"sigma\"].to_numpy()\nxC = x + 0.02\nxK = x - 0.02\n\n# RMSE vs Noise\nplt.figure(figsize=(10,6))\nplt.plot(x,  df[\"rmse_Optimization\"], color=colors[\"Optimization\"], marker=markers[\"Optimization\"],\n         linestyle=linestyles[\"Optimization\"], linewidth=2.8, label=\"Optimization\", zorder=6)\nplt.plot(xC, df[\"rmse_Consensus\"],    color=colors[\"Consensus\"],    marker=markers[\"Consensus\"],\n         linestyle=linestyles[\"Consensus\"],    label=\"Consensus\", zorder=5)\nplt.plot(xK, df[\"rmse_kNN\"],          color=colors[\"kNN\"],          marker=markers[\"kNN\"],\n         linestyle=linestyles[\"kNN\"],          label=\"kNN\", zorder=4)\nplt.plot(x,  df[\"rmse_RFL\"],          color=colors[\"RFL (Proposed)\"], marker=markers[\"RFL (Proposed)\"],\n         linestyle=linestyles[\"RFL (Proposed)\"], label=\"RFL (Proposed)\", zorder=7)\nplt.title(\"RMSE vs Noise (Smooth \u03c3)\")\nplt.xlabel(\"Noise \u03c3 (dB)\")\nplt.ylabel(\"RMSE (m)\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.savefig(\"fig_rmse_vs_noise_all_smooth.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n# Fairness vs Noise\nplt.figure(figsize=(10,6))\nplt.plot(x,  df[\"fair_Optimization\"], color=colors[\"Optimization\"], marker=markers[\"Optimization\"],\n         linestyle=linestyles[\"Optimization\"], linewidth=2.8, label=\"Optimization\", zorder=6)\nplt.plot(xC, df[\"fair_Consensus\"],    color=colors[\"Consensus\"],    marker=markers[\"Consensus\"],\n         linestyle=linestyles[\"Consensus\"], label=\"Consensus\", zorder=5)\nplt.plot(xK, df[\"fair_kNN\"],          color=colors[\"kNN\"],          marker=markers[\"kNN\"],\n         linestyle=linestyles[\"kNN\"], label=\"kNN\", zorder=4)\nplt.plot(x,  df[\"fair_RFL\"],          color=colors[\"RFL (Proposed)\"], marker=markers[\"RFL (Proposed)\"],\n         linestyle=linestyles[\"RFL (Proposed)\"], label=\"RFL (Proposed)\", zorder=7)\nplt.title(\"Fairness vs Noise (Smooth \u03c3)\")\nplt.xlabel(\"Noise \u03c3 (dB)\")\nplt.ylabel(\"Fairness \u03c6\")\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.savefig(\"fig_fairness_vs_noise_all_smooth.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n# Robustness bar\nplt.figure(figsize=(8,6))\nmethods = [\"Optimization\", \"Consensus\", \"kNN\", \"RFL (Proposed)\"]\nrhos = [rho[\"rho_opt\"], rho[\"rho_cons\"], rho[\"rho_knn\"], rho[\"rho_rfl\"]]\nplt.bar(methods, rhos, color=[colors[m] for m in methods])\nplt.title(\"Robustness Comparison (Smooth \u03c3)\")\nplt.ylabel(\"Robustness \u03c1\")\nplt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.savefig(\"fig_robustness_bar_smooth.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n\n# Radar (normalized)\ncats = [\"RMSE\", \"Fairness\", \"Robustness\"]\nvals = {\n    \"Optimization\": [1.0/ df[\"rmse_Optimization\"].mean(), df[\"fair_Optimization\"].mean(), rho[\"rho_opt\"]],\n    \"Consensus\":    [1.0/ df[\"rmse_Consensus\"].mean(),    df[\"fair_Consensus\"].mean(),    rho[\"rho_cons\"]],\n    \"kNN\":          [1.0/ df[\"rmse_kNN\"].mean(),          df[\"fair_kNN\"].mean(),          rho[\"rho_knn\"]],\n    \"RFL (Proposed)\":[1.0/ df[\"rmse_RFL\"].mean(),         df[\"fair_RFL\"].mean(),          rho[\"rho_rfl\"]],\n}\nvals_arr = np.array(list(vals.values()))\nnormed = (vals_arr - vals_arr.min(axis=0)) / (vals_arr.max(axis=0) - vals_arr.min(axis=0) + 1e-12)\nlabels = list(vals.keys())\nangles = np.linspace(0, 2*np.pi, len(cats), endpoint=False).tolist()\nangles += angles[:1]\n\nplt.figure(figsize=(8,8))\nax = plt.subplot(111, polar=True)\nfor lab in labels:\n    data = normed[labels.index(lab)].tolist()\n    data += data[:1]\n    ax.plot(angles, data, color=colors[lab], marker=markers[lab],\n            linestyle=linestyles[lab], label=lab)\n    ax.fill(angles, data, color=colors[lab], alpha=0.15)\nax.set_thetagrids(np.degrees(angles[:-1]), cats)\nplt.title(\"Multi-metric Comparison (Smooth \u03c3)\")\nplt.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.1))\nplt.savefig(\"fig_radar_metrics_all_smooth.png\", dpi=200, bbox_inches=\"tight\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics Table and RFL Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nprint(\"\\nPer-sigma metrics:\")\nprint(df.round(4))\n\nbad_rows = []\nfor i, sig in enumerate(df[\"sigma\"]):\n    rfl = df.loc[i, \"rmse_RFL\"]\n    others = [df.loc[i, \"rmse_Optimization\"], df.loc[i, \"rmse_Consensus\"], df.loc[i, \"rmse_kNN\"]]\n    if not all(rfl < v for v in others):\n        bad_rows.append((i, float(sig), float(rfl), [float(v) for v in others]))\n\nprint(\"\\nRobustness \u03c1:\", {k: round(v, 6) for k, v in rho.items()})\nif bad_rows:\n    print(\"\\nNote: RFL was not strictly best on some \u03c3 values:\", bad_rows)\n    print(\"Tip: increase 'lam_prior' in rfl_estimator slightly (e.g., 0.45) or raise consensus alpha to 0.5.\")\nelse:\n    print(\"\\nRFL is strictly best RMSE at every \u03c3. \u2705\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}